{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.test.gpu_device_name()\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess:  <tensorflow.python.client.session.Session object at 0x7f1e41404be0>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "print('sess: ', sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14412728677005785552\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11195556912945122532\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11329617920\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5173312685727779917\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11901649289995669639\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atriadplt2/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0.0\n"
     ]
    }
   ],
   "source": [
    "import notebook\n",
    "print(notebook.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 24, 24, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_convolution (Conv2D)      (None, 12, 12, 64)   9472        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (MaxPooling2D)            (None, 6, 6, 64)     0           conv1_convolution[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_convolution (Conv2D)      (None, 3, 3, 192)    110784      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (MaxPooling2D)            (None, 3, 3, 192)    0           conv2_convolution[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception3a_activation  (None, 3, 3, 64)     12352       conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception3a_activation  (None, 3, 3, 64)     12352       conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception3a_activation  (None, 3, 3, 192)    0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception3a_activation ( (None, 3, 3, 64)     12352       conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception3a_activation  (None, 3, 3, 64)     36928       tower1a_inception3a_activation[0]\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception3a_activation  (None, 3, 3, 64)     102464      tower2a_inception3a_activation[0]\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception3a_activation  (None, 3, 3, 64)     12352       tower_3ainception3a_activation[0]\n",
      "__________________________________________________________________________________________________\n",
      "concenation_inception3a_activat (None, 3, 3, 256)    0           tower0_inception3a_activation[0][\n",
      "                                                                 tower1b_inception3a_activation[0]\n",
      "                                                                 tower2b_inception3a_activation[0]\n",
      "                                                                 tower3b_inception3a_activation[0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 3, 3, 256)    0           concenation_inception3a_activatio\n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception3 (Conv2D)     (None, 3, 3, 120)    30840       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception3 (Conv2D)     (None, 3, 3, 120)    30840       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception3 (MaxPooling2 (None, 3, 3, 256)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception3 (Conv2D)      (None, 3, 3, 120)    30840       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception3 (Conv2D)     (None, 3, 3, 120)    129720      tower1a_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception3 (Conv2D)     (None, 3, 3, 120)    360120      tower2a_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception3 (Conv2D)     (None, 3, 3, 120)    30840       tower_3ainception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 3, 3, 480)    0           tower0_inception3[0][0]          \n",
      "                                                                 tower1b_inception3[0][0]         \n",
      "                                                                 tower2b_inception3[0][0]         \n",
      "                                                                 tower3b_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 3, 3, 480)    0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pooling_inception3 (MaxPooling2 (None, 3, 3, 480)    0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception4a_activation_ (None, 3, 3, 128)    61568       pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception4a_activation_ (None, 3, 3, 128)    61568       pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception4a_activation_ (None, 3, 3, 480)    0           pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception4b_activation_ (None, 3, 3, 128)    61568       pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception4b_activation_ (None, 3, 3, 128)    61568       pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception4b_activation_ (None, 3, 3, 480)    0           pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception4c_activation_ (None, 3, 3, 128)    61568       pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception4c_activation_ (None, 3, 3, 128)    61568       pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception4c_activation_ (None, 3, 3, 480)    0           pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception4a_activation_t (None, 3, 3, 128)    61568       pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception4a_activation_ (None, 3, 3, 128)    147584      tower1a_inception4a_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception4a_activation_ (None, 3, 3, 128)    409728      tower2a_inception4a_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception4a_activation_ (None, 3, 3, 128)    61568       tower_3ainception4a_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception4b_activation_b (None, 3, 3, 128)    61568       pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception4b_activation_ (None, 3, 3, 128)    147584      tower1a_inception4b_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception4b_activation_ (None, 3, 3, 128)    409728      tower2a_inception4b_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception4b_activation_ (None, 3, 3, 128)    61568       tower_3ainception4b_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception4c_activation_b (None, 3, 3, 128)    61568       pooling_inception3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception4c_activation_ (None, 3, 3, 128)    147584      tower1a_inception4c_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception4c_activation_ (None, 3, 3, 128)    409728      tower2a_inception4c_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception4c_activation_ (None, 3, 3, 128)    61568       tower_3ainception4c_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "concenation_inception4a_activat (None, 3, 3, 512)    0           tower0_inception4a_activation_tru\n",
      "                                                                 tower1b_inception4a_activation_tr\n",
      "                                                                 tower2b_inception4a_activation_tr\n",
      "                                                                 tower3b_inception4a_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "concenation_inception4b_activat (None, 3, 3, 512)    0           tower0_inception4b_activation_bra\n",
      "                                                                 tower1b_inception4b_activation_br\n",
      "                                                                 tower2b_inception4b_activation_br\n",
      "                                                                 tower3b_inception4b_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "concenation_inception4c_activat (None, 3, 3, 512)    0           tower0_inception4c_activation_bra\n",
      "                                                                 tower1b_inception4c_activation_br\n",
      "                                                                 tower2b_inception4c_activation_br\n",
      "                                                                 tower3b_inception4c_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 3, 3, 512)    0           concenation_inception4a_activatio\n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 3, 3, 512)    0           concenation_inception4b_activatio\n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 3, 3, 512)    0           concenation_inception4c_activatio\n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception4e_activation_ (None, 3, 3, 132)    67716       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception4e_activation_ (None, 3, 3, 132)    67716       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception4e_activation_ (None, 3, 3, 512)    0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception4e_activation_ (None, 3, 3, 132)    67716       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception4e_activation_ (None, 3, 3, 132)    67716       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception4e_activation_ (None, 3, 3, 512)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception4e_activation_ (None, 3, 3, 132)    67716       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception4e_activation_ (None, 3, 3, 132)    67716       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception4e_activation_ (None, 3, 3, 512)    0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception4e_activation_t (None, 3, 3, 132)    67716       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception4e_activation_ (None, 3, 3, 132)    156948      tower1a_inception4e_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception4e_activation_ (None, 3, 3, 132)    435732      tower2a_inception4e_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception4e_activation_ (None, 3, 3, 132)    67716       tower_3ainception4e_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception4e_activation_b (None, 3, 3, 132)    67716       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception4e_activation_ (None, 3, 3, 132)    156948      tower1a_inception4e_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception4e_activation_ (None, 3, 3, 132)    435732      tower2a_inception4e_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception4e_activation_ (None, 3, 3, 132)    67716       tower_3ainception4e_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception4e_activation_b (None, 3, 3, 132)    67716       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception4e_activation_ (None, 3, 3, 132)    156948      tower1a_inception4e_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception4e_activation_ (None, 3, 3, 132)    435732      tower2a_inception4e_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception4e_activation_ (None, 3, 3, 132)    67716       tower_3ainception4e_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "concenation_inception4e_activat (None, 3, 3, 528)    0           tower0_inception4e_activation_tru\n",
      "                                                                 tower1b_inception4e_activation_tr\n",
      "                                                                 tower2b_inception4e_activation_tr\n",
      "                                                                 tower3b_inception4e_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "concenation_inception4e_activat (None, 3, 3, 528)    0           tower0_inception4e_activation_bra\n",
      "                                                                 tower1b_inception4e_activation_br\n",
      "                                                                 tower2b_inception4e_activation_br\n",
      "                                                                 tower3b_inception4e_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "concenation_inception4e_activat (None, 3, 3, 528)    0           tower0_inception4e_activation_bra\n",
      "                                                                 tower1b_inception4e_activation_br\n",
      "                                                                 tower2b_inception4e_activation_br\n",
      "                                                                 tower3b_inception4e_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 3, 3, 528)    0           concenation_inception4e_activatio\n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 3, 3, 528)    0           concenation_inception4e_activatio\n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 3, 3, 528)    0           concenation_inception4e_activatio\n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception4_trunk (Conv2 (None, 3, 3, 208)    110032      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception4_trunk (Conv2 (None, 3, 3, 208)    110032      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception4_trunk (MaxPo (None, 3, 3, 528)    0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception4_branch_1 (Co (None, 3, 3, 208)    110032      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception4_branch_1 (Co (None, 3, 3, 208)    110032      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception4_branch_1 (Ma (None, 3, 3, 528)    0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception4_branch_2 (Co (None, 3, 3, 208)    110032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception4_branch_2 (Co (None, 3, 3, 208)    110032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception4_branch_2 (Ma (None, 3, 3, 528)    0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception4_trunk (Conv2D (None, 3, 3, 208)    110032      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception4_trunk (Conv2 (None, 3, 3, 208)    389584      tower1a_inception4_trunk[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception4_trunk (Conv2 (None, 3, 3, 208)    1081808     tower2a_inception4_trunk[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception4_trunk (Conv2 (None, 3, 3, 208)    110032      tower_3ainception4_trunk[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception4_branch_1 (Con (None, 3, 3, 208)    110032      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception4_branch_1 (Co (None, 3, 3, 208)    389584      tower1a_inception4_branch_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception4_branch_1 (Co (None, 3, 3, 208)    1081808     tower2a_inception4_branch_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception4_branch_1 (Co (None, 3, 3, 208)    110032      tower_3ainception4_branch_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception4_branch_2 (Con (None, 3, 3, 208)    110032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception4_branch_2 (Co (None, 3, 3, 208)    389584      tower1a_inception4_branch_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception4_branch_2 (Co (None, 3, 3, 208)    1081808     tower2a_inception4_branch_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception4_branch_2 (Co (None, 3, 3, 208)    110032      tower_3ainception4_branch_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 3, 3, 832)    0           tower0_inception4_trunk[0][0]    \n",
      "                                                                 tower1b_inception4_trunk[0][0]   \n",
      "                                                                 tower2b_inception4_trunk[0][0]   \n",
      "                                                                 tower3b_inception4_trunk[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 3, 3, 832)    0           tower0_inception4_branch_1[0][0] \n",
      "                                                                 tower1b_inception4_branch_1[0][0]\n",
      "                                                                 tower2b_inception4_branch_1[0][0]\n",
      "                                                                 tower3b_inception4_branch_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 3, 3, 832)    0           tower0_inception4_branch_2[0][0] \n",
      "                                                                 tower1b_inception4_branch_2[0][0]\n",
      "                                                                 tower2b_inception4_branch_2[0][0]\n",
      "                                                                 tower3b_inception4_branch_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 3, 3, 832)    0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 3, 3, 832)    0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 3, 3, 832)    0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pooling_inception4_trunk (MaxPo (None, 3, 3, 832)    0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pooling_inception4_branch_1 (Ma (None, 3, 3, 832)    0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pooling_inception4_branch_2 (Ma (None, 3, 3, 832)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception5a_activation_ (None, 3, 3, 208)    173264      pooling_inception4_trunk[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception5a_activation_ (None, 3, 3, 208)    173264      pooling_inception4_trunk[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception5a_activation_ (None, 3, 3, 832)    0           pooling_inception4_trunk[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception5a_activation_ (None, 3, 3, 208)    173264      pooling_inception4_branch_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception5a_activation_ (None, 3, 3, 208)    173264      pooling_inception4_branch_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception5a_activation_ (None, 3, 3, 832)    0           pooling_inception4_branch_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception5a_activation_ (None, 3, 3, 208)    173264      pooling_inception4_branch_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception5a_activation_ (None, 3, 3, 208)    173264      pooling_inception4_branch_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception5a_activation_ (None, 3, 3, 832)    0           pooling_inception4_branch_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception5a_activation_t (None, 3, 3, 208)    173264      pooling_inception4_trunk[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception5a_activation_ (None, 3, 3, 208)    389584      tower1a_inception5a_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception5a_activation_ (None, 3, 3, 208)    1081808     tower2a_inception5a_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception5a_activation_ (None, 3, 3, 208)    173264      tower_3ainception5a_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception5a_activation_b (None, 3, 3, 208)    173264      pooling_inception4_branch_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception5a_activation_ (None, 3, 3, 208)    389584      tower1a_inception5a_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception5a_activation_ (None, 3, 3, 208)    1081808     tower2a_inception5a_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception5a_activation_ (None, 3, 3, 208)    173264      tower_3ainception5a_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception5a_activation_b (None, 3, 3, 208)    173264      pooling_inception4_branch_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception5a_activation_ (None, 3, 3, 208)    389584      tower1a_inception5a_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception5a_activation_ (None, 3, 3, 208)    1081808     tower2a_inception5a_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception5a_activation_ (None, 3, 3, 208)    173264      tower_3ainception5a_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "concenation_inception5a_activat (None, 3, 3, 832)    0           tower0_inception5a_activation_tru\n",
      "                                                                 tower1b_inception5a_activation_tr\n",
      "                                                                 tower2b_inception5a_activation_tr\n",
      "                                                                 tower3b_inception5a_activation_tr\n",
      "__________________________________________________________________________________________________\n",
      "concenation_inception5a_activat (None, 3, 3, 832)    0           tower0_inception5a_activation_bra\n",
      "                                                                 tower1b_inception5a_activation_br\n",
      "                                                                 tower2b_inception5a_activation_br\n",
      "                                                                 tower3b_inception5a_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "concenation_inception5a_activat (None, 3, 3, 832)    0           tower0_inception5a_activation_bra\n",
      "                                                                 tower1b_inception5a_activation_br\n",
      "                                                                 tower2b_inception5a_activation_br\n",
      "                                                                 tower3b_inception5a_activation_br\n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 3, 3, 832)    0           concenation_inception5a_activatio\n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 3, 3, 832)    0           concenation_inception5a_activatio\n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 3, 3, 832)    0           concenation_inception5a_activatio\n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception5b_1_trunk (Co (None, 3, 3, 256)    213248      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception5b_1_trunk (Co (None, 3, 3, 256)    213248      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception5b_1_trunk (Ma (None, 3, 3, 832)    0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception5b_2_branch_1  (None, 3, 3, 256)    213248      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception5b_2_branch_1  (None, 3, 3, 256)    213248      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception5b_2_branch_1  (None, 3, 3, 832)    0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1a_inception5b_3_branch_2  (None, 3, 3, 256)    213248      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower2a_inception5b_3_branch_2  (None, 3, 3, 256)    213248      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower_3ainception5b_3_branch_2  (None, 3, 3, 832)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception5b_1_trunk (Con (None, 3, 3, 256)    213248      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception5b_1_trunk (Co (None, 3, 3, 256)    590080      tower1a_inception5b_1_trunk[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception5b_1_trunk (Co (None, 3, 3, 256)    1638656     tower2a_inception5b_1_trunk[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception5b_1_trunk (Co (None, 3, 3, 256)    213248      tower_3ainception5b_1_trunk[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception5b_2_branch_1 ( (None, 3, 3, 256)    213248      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception5b_2_branch_1  (None, 3, 3, 256)    590080      tower1a_inception5b_2_branch_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception5b_2_branch_1  (None, 3, 3, 256)    1638656     tower2a_inception5b_2_branch_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception5b_2_branch_1  (None, 3, 3, 256)    213248      tower_3ainception5b_2_branch_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "tower0_inception5b_3_branch_2 ( (None, 3, 3, 256)    213248      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tower1b_inception5b_3_branch_2  (None, 3, 3, 256)    590080      tower1a_inception5b_3_branch_2[0]\n",
      "__________________________________________________________________________________________________\n",
      "tower2b_inception5b_3_branch_2  (None, 3, 3, 256)    1638656     tower2a_inception5b_3_branch_2[0]\n",
      "__________________________________________________________________________________________________\n",
      "tower3b_inception5b_3_branch_2  (None, 3, 3, 256)    213248      tower_3ainception5b_3_branch_2[0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 3, 3, 1024)   0           tower0_inception5b_1_trunk[0][0] \n",
      "                                                                 tower1b_inception5b_1_trunk[0][0]\n",
      "                                                                 tower2b_inception5b_1_trunk[0][0]\n",
      "                                                                 tower3b_inception5b_1_trunk[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 3, 3, 1024)   0           tower0_inception5b_2_branch_1[0][\n",
      "                                                                 tower1b_inception5b_2_branch_1[0]\n",
      "                                                                 tower2b_inception5b_2_branch_1[0]\n",
      "                                                                 tower3b_inception5b_2_branch_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 3, 3, 1024)   0           tower0_inception5b_3_branch_2[0][\n",
      "                                                                 tower1b_inception5b_3_branch_2[0]\n",
      "                                                                 tower2b_inception5b_3_branch_2[0]\n",
      "                                                                 tower3b_inception5b_3_branch_2[0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 3, 3, 1024)   0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 3, 3, 1024)   0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 3, 3, 1024)   0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pooling_inception5b_1_trunk (Ma (None, 3, 3, 1024)   0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pooling_inception5b_2_branch_1  (None, 3, 3, 1024)   0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pooling_inception5b_3_branch_2  (None, 3, 3, 1024)   0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 9, 3, 1024)   0           pooling_inception5b_1_trunk[0][0]\n",
      "                                                                 pooling_inception5b_2_branch_1[0]\n",
      "                                                                 pooling_inception5b_3_branch_2[0]\n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 27648)        0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           1769536     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1591)         103415      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 1591)         0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 29,269,679\n",
      "Trainable params: 29,269,679\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 424961 images belonging to 1591 classes.\n",
      "Found 120531 images belonging to 1591 classes.\n",
      "training: \n",
      "Epoch 1/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.3648 - acc: 0.0082\n",
      "Epoch 00001: val_loss improved from inf to 7.35718, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 7814s 588ms/step - loss: 7.3648 - acc: 0.0082 - val_loss: 7.3572 - val_acc: 0.0088\n",
      "Epoch 2/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.3503 - acc: 0.0101\n",
      "Epoch 00002: val_loss improved from 7.35718 to 7.34295, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2339s 176ms/step - loss: 7.3503 - acc: 0.0101 - val_loss: 7.3429 - val_acc: 0.0088\n",
      "Epoch 3/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.3366 - acc: 0.0100\n",
      "Epoch 00003: val_loss improved from 7.34295 to 7.32939, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2339s 176ms/step - loss: 7.3366 - acc: 0.0100 - val_loss: 7.3294 - val_acc: 0.0088\n",
      "Epoch 4/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.3235 - acc: 0.0100\n",
      "Epoch 00004: val_loss improved from 7.32939 to 7.31646, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2346s 177ms/step - loss: 7.3235 - acc: 0.0100 - val_loss: 7.3165 - val_acc: 0.0088\n",
      "Epoch 5/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.3110 - acc: 0.0100\n",
      "Epoch 00005: val_loss improved from 7.31646 to 7.30412, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2336s 176ms/step - loss: 7.3110 - acc: 0.0100 - val_loss: 7.3041 - val_acc: 0.0088\n",
      "Epoch 6/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.2991 - acc: 0.0100\n",
      "Epoch 00006: val_loss improved from 7.30412 to 7.29235, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2339s 176ms/step - loss: 7.2991 - acc: 0.0100 - val_loss: 7.2924 - val_acc: 0.0088\n",
      "Epoch 7/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.2878 - acc: 0.0100\n",
      "Epoch 00007: val_loss improved from 7.29235 to 7.28110, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2342s 176ms/step - loss: 7.2878 - acc: 0.0100 - val_loss: 7.2811 - val_acc: 0.0088\n",
      "Epoch 8/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.2769 - acc: 0.0100\n",
      "Epoch 00008: val_loss improved from 7.28110 to 7.27035, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2342s 176ms/step - loss: 7.2769 - acc: 0.0100 - val_loss: 7.2704 - val_acc: 0.0088\n",
      "Epoch 9/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.2665 - acc: 0.0100\n",
      "Epoch 00009: val_loss improved from 7.27035 to 7.26008, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2340s 176ms/step - loss: 7.2665 - acc: 0.0100 - val_loss: 7.2601 - val_acc: 0.0088\n",
      "Epoch 10/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.2566 - acc: 0.0100\n",
      "Epoch 00010: val_loss improved from 7.26008 to 7.25025, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2337s 176ms/step - loss: 7.2566 - acc: 0.0100 - val_loss: 7.2502 - val_acc: 0.0088\n",
      "Epoch 11/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.2471 - acc: 0.0100\n",
      "Epoch 00011: val_loss improved from 7.25025 to 7.24085, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2344s 177ms/step - loss: 7.2471 - acc: 0.0100 - val_loss: 7.2408 - val_acc: 0.0088\n",
      "Epoch 12/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.2380 - acc: 0.0100\n",
      "Epoch 00012: val_loss improved from 7.24085 to 7.23184, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2341s 176ms/step - loss: 7.2380 - acc: 0.0100 - val_loss: 7.2318 - val_acc: 0.0088\n",
      "Epoch 13/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.2294 - acc: 0.0100\n",
      "Epoch 00013: val_loss improved from 7.23184 to 7.22322, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2343s 176ms/step - loss: 7.2293 - acc: 0.0100 - val_loss: 7.2232 - val_acc: 0.0088\n",
      "Epoch 14/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.2210 - acc: 0.0100\n",
      "Epoch 00014: val_loss improved from 7.22322 to 7.21496, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2342s 176ms/step - loss: 7.2210 - acc: 0.0100 - val_loss: 7.2150 - val_acc: 0.0088\n",
      "Epoch 15/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.2130 - acc: 0.0100\n",
      "Epoch 00015: val_loss improved from 7.21496 to 7.20704, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2340s 176ms/step - loss: 7.2130 - acc: 0.0100 - val_loss: 7.2070 - val_acc: 0.0088\n",
      "Epoch 16/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.2054 - acc: 0.0100\n",
      "Epoch 00016: val_loss improved from 7.20704 to 7.19945, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2340s 176ms/step - loss: 7.2054 - acc: 0.0100 - val_loss: 7.1994 - val_acc: 0.0088\n",
      "Epoch 17/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1980 - acc: 0.0100\n",
      "Epoch 00017: val_loss improved from 7.19945 to 7.19217, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2342s 176ms/step - loss: 7.1980 - acc: 0.0100 - val_loss: 7.1922 - val_acc: 0.0088\n",
      "Epoch 18/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1911 - acc: 0.0100\n",
      "Epoch 00018: val_loss improved from 7.19217 to 7.18518, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2342s 176ms/step - loss: 7.1911 - acc: 0.0100 - val_loss: 7.1852 - val_acc: 0.0088\n",
      "Epoch 19/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1843 - acc: 0.0100\n",
      "Epoch 00019: val_loss improved from 7.18518 to 7.17847, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2344s 176ms/step - loss: 7.1843 - acc: 0.0100 - val_loss: 7.1785 - val_acc: 0.0088\n",
      "Epoch 20/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1778 - acc: 0.0100\n",
      "Epoch 00020: val_loss improved from 7.17847 to 7.17203, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2344s 177ms/step - loss: 7.1778 - acc: 0.0100 - val_loss: 7.1720 - val_acc: 0.0088\n",
      "Epoch 21/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1716 - acc: 0.0100\n",
      "Epoch 00021: val_loss improved from 7.17203 to 7.16584, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2341s 176ms/step - loss: 7.1716 - acc: 0.0100 - val_loss: 7.1658 - val_acc: 0.0088\n",
      "Epoch 22/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1656 - acc: 0.0100\n",
      "Epoch 00022: val_loss improved from 7.16584 to 7.15989, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2348s 177ms/step - loss: 7.1656 - acc: 0.0100 - val_loss: 7.1599 - val_acc: 0.0088\n",
      "Epoch 23/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1599 - acc: 0.0100\n",
      "Epoch 00023: val_loss improved from 7.15989 to 7.15417, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2346s 177ms/step - loss: 7.1599 - acc: 0.0100 - val_loss: 7.1542 - val_acc: 0.0088\n",
      "Epoch 24/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1544 - acc: 0.0100\n",
      "Epoch 00024: val_loss improved from 7.15417 to 7.14866, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2341s 176ms/step - loss: 7.1544 - acc: 0.0100 - val_loss: 7.1487 - val_acc: 0.0088\n",
      "Epoch 25/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1491 - acc: 0.0100\n",
      "Epoch 00025: val_loss improved from 7.14866 to 7.14336, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2346s 177ms/step - loss: 7.1491 - acc: 0.0100 - val_loss: 7.1434 - val_acc: 0.0088\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1440 - acc: 0.0100\n",
      "Epoch 00026: val_loss improved from 7.14336 to 7.13826, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2346s 177ms/step - loss: 7.1440 - acc: 0.0100 - val_loss: 7.1383 - val_acc: 0.0088\n",
      "Epoch 27/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1391 - acc: 0.0100\n",
      "Epoch 00027: val_loss improved from 7.13826 to 7.13334, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2342s 176ms/step - loss: 7.1391 - acc: 0.0100 - val_loss: 7.1333 - val_acc: 0.0088\n",
      "Epoch 28/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1343 - acc: 0.0100\n",
      "Epoch 00028: val_loss improved from 7.13334 to 7.12860, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2342s 176ms/step - loss: 7.1343 - acc: 0.0100 - val_loss: 7.1286 - val_acc: 0.0088\n",
      "Epoch 29/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1298 - acc: 0.0100\n",
      "Epoch 00029: val_loss improved from 7.12860 to 7.12403, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2340s 176ms/step - loss: 7.1298 - acc: 0.0100 - val_loss: 7.1240 - val_acc: 0.0088\n",
      "Epoch 30/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1254 - acc: 0.0100\n",
      "Epoch 00030: val_loss improved from 7.12403 to 7.11961, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2347s 177ms/step - loss: 7.1254 - acc: 0.0100 - val_loss: 7.1196 - val_acc: 0.0088\n",
      "Epoch 31/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1211 - acc: 0.0100\n",
      "Epoch 00031: val_loss improved from 7.11961 to 7.11535, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2346s 177ms/step - loss: 7.1211 - acc: 0.0100 - val_loss: 7.1153 - val_acc: 0.0088\n",
      "Epoch 32/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1171 - acc: 0.0100\n",
      "Epoch 00032: val_loss improved from 7.11535 to 7.11124, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2346s 177ms/step - loss: 7.1171 - acc: 0.0100 - val_loss: 7.1112 - val_acc: 0.0088\n",
      "Epoch 33/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1131 - acc: 0.0100\n",
      "Epoch 00033: val_loss improved from 7.11124 to 7.10726, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2344s 176ms/step - loss: 7.1131 - acc: 0.0100 - val_loss: 7.1073 - val_acc: 0.0088\n",
      "Epoch 34/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1092 - acc: 0.0100\n",
      "Epoch 00034: val_loss improved from 7.10726 to 7.10341, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2345s 177ms/step - loss: 7.1092 - acc: 0.0100 - val_loss: 7.1034 - val_acc: 0.0088\n",
      "Epoch 35/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1055 - acc: 0.0100\n",
      "Epoch 00035: val_loss improved from 7.10341 to 7.09969, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2344s 176ms/step - loss: 7.1055 - acc: 0.0100 - val_loss: 7.0997 - val_acc: 0.0088\n",
      "Epoch 36/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.1021 - acc: 0.0100\n",
      "Epoch 00036: val_loss improved from 7.09969 to 7.09609, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2352s 177ms/step - loss: 7.1021 - acc: 0.0100 - val_loss: 7.0961 - val_acc: 0.0088\n",
      "Epoch 37/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.0985 - acc: 0.0100\n",
      "Epoch 00037: val_loss improved from 7.09609 to 7.09260, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2355s 177ms/step - loss: 7.0985 - acc: 0.0100 - val_loss: 7.0926 - val_acc: 0.0088\n",
      "Epoch 38/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.0952 - acc: 0.0100\n",
      "Epoch 00038: val_loss improved from 7.09260 to 7.08922, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2351s 177ms/step - loss: 7.0952 - acc: 0.0100 - val_loss: 7.0892 - val_acc: 0.0088\n",
      "Epoch 39/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.0919 - acc: 0.0100\n",
      "Epoch 00039: val_loss improved from 7.08922 to 7.08595, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2345s 177ms/step - loss: 7.0919 - acc: 0.0100 - val_loss: 7.0859 - val_acc: 0.0088\n",
      "Epoch 40/40\n",
      "13279/13280 [============================>.] - ETA: 0s - loss: 7.0888 - acc: 0.0100\n",
      "Epoch 00040: val_loss improved from 7.08595 to 7.08277, saving model to tbe_cnn_ytd_epoch_100.h5\n",
      "13280/13280 [==============================] - 2351s 177ms/step - loss: 7.0888 - acc: 0.0100 - val_loss: 7.0828 - val_acc: 0.0088\n",
      "Common Function\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import model_2_branch\n",
    "import common_function\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "import os\n",
    "import asyncio\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from tensorflow.python.keras.models import load_model, Model\n",
    "\n",
    "# Just disables the warning, doesn't enable AVX/FMA (no GPU)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "epochs = 40\n",
    "l_rate = 1.0e-4\n",
    "decay = l_rate / epochs\n",
    "sgd = SGD(lr=l_rate, momentum=0.9, decay=decay, nesterov=False)\n",
    "batch_size = 32\n",
    "img_width, img_height = 24, 24\n",
    "path_data_set = './ytd'\n",
    "input_img, merged = model_2_branch.get_model(img_width, img_height)\n",
    "num_train_images = 424961  # training images: 424961  # total images: 605855\n",
    "file_path = 'tbe_cnn_2_branch.h5'\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "\n",
    "############################################### Training Dataset #############################################################\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    flatten = Flatten()(merged)\n",
    "    dense = Dense(64)(flatten)\n",
    "    activation = Activation('softmax')(dense)\n",
    "    dropout = Dropout(0.5)(activation)\n",
    "    dense = Dense(1591)(dropout)\n",
    "    activation = Activation('softmax')(dense)\n",
    "\n",
    "    base_model = Model(input_img, activation)\n",
    "else:\n",
    "    base_model = load_model(file_path)\n",
    "    # base_model.load_weights(file_path)\n",
    "\n",
    "base_model.summary()\n",
    "\n",
    "train_generator_lr = datagen.flow_from_directory(\n",
    "    str(path_data_set + '/train'),\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator_lr = datagen.flow_from_directory(\n",
    "    str(path_data_set + '/validate'),\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print('training: ')\n",
    "\n",
    "base_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    file_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='auto',\n",
    "    verbose=1\n",
    ")\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = base_model.fit_generator(\n",
    "    generator=train_generator_lr,\n",
    "    steps_per_epoch=num_train_images // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator_lr,\n",
    "    validation_steps=800 // batch_size,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "common_func = common_function.CommonFunction()\n",
    "common_func.plot_training(history, 'TBE-CNN (2 Branch)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('ytd.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
